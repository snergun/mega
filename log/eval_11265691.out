usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'deactivate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'build', 'content-trust', 'convert', 'debug', 'develop', 'doctor', 'index', 'inspect', 'metapackage', 'render', 'repoquery', 'skeleton', 'token', 'verify', 'repo', 'env', 'server')
Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
2025-07-30 15:40:45 | INFO | fairseq_cli.eval_lm | Namespace(add_bos_token=False, all_gather_list_size=16384, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', chunk_nums=2, context_window=0, cpu=False, criterion='cross_entropy', data='data/wikitext-103', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', empty_cache_freq=0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, gen_subset='test', localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, max_sentences=None, max_target_positions=None, max_tokens=5000000, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_overrides='{"decoder_chunk_size": 2048,"max_tokens_valid": 5000000}', model_parallel_size=1, no_progress_bar=False, no_seed_provided=True, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer=None, output_dictionary_size=-1, output_word_probs=False, output_word_stats=False, past_target=False, path='{SAVE}/model.pt', profile=False, quantization_config_path=None, quiet=False, remove_bpe=None, required_batch_size_multiple=8, results_path=None, sample_break_mode='complete', seed=1, self_target=False, shard_id=0, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, softmax_batch=1024, task='language_modeling', tensorboard_logdir='', test_chunk_size=2048, threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, tpu=False, user_dir=None, valid_block='splits:10', variant_block_multiple_max=1, variant_block_multiple_min=1, wandb_entity=None, wandb_id=None, wandb_project=None, warmup_updates=0)
2025-07-30 15:40:46 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types
2025-07-30 15:40:46 | INFO | fairseq_cli.eval_lm | loading model(s) from {SAVE}/model.pt
Traceback (most recent call last):
  File "fairseq_cli/eval_mega_lm.py", line 288, in <module>
    cli_main()
  File "fairseq_cli/eval_mega_lm.py", line 284, in cli_main
    distributed_utils.call_main(args, main)
  File "/work/hdd/bcyi/eergun/mega/fairseq/distributed_utils.py", line 193, in call_main
    main(args, **kwargs)
  File "fairseq_cli/eval_mega_lm.py", line 79, in main
    models, args = checkpoint_utils.load_model_ensemble(
  File "/work/hdd/bcyi/eergun/mega/fairseq/checkpoint_utils.py", line 221, in load_model_ensemble
    ensemble, args, _task = load_model_ensemble_and_task(
  File "/work/hdd/bcyi/eergun/mega/fairseq/checkpoint_utils.py", line 234, in load_model_ensemble_and_task
    raise IOError("Model file not found: {}".format(filename))
OSError: Model file not found: {SAVE}/model.pt
